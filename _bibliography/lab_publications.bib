
@phdthesis{landgraf_robobee:_2013,
	title = {{RoboBee}: {A} {Biomimetic} {Honeybee} {Robot} for the {Analysis} of the {Dance} {Communication} {System}},
	shorttitle = {{RoboBee}},
	url = {http://www.diss.fu-berlin.de/diss/receive/FUDISS_thesis_000000094818?lang=de},
	urldate = {2016-11-29},
	school = {Berlin, Freie Universität Berlin, 2013},
	author = {Landgraf, Tim},
	year = {2013}
}

@inproceedings{landgraf_multi-agent_2012,
	title = {A {Multi}-agent {Platform} for {Biomimetic} {Fish}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-31525-1_44},
	urldate = {2016-11-29},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Landgraf, Tim and Akkad, Rami and Nguyen, Hai and Clément, Romain O. and Krause, Jens and Rojas, Raúl},
	year = {2012},
	keywords = {peer-reviewed},
	pages = {365--366}
}

@inproceedings{helgadottir_conditioned_2013,
	title = {Conditioned behavior in a robot controlled by a spiking neural network},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6696078},
	urldate = {2016-11-29},
	booktitle = {Neural {Engineering} ({NER}), 2013 6th {International} {IEEE}/{EMBS} {Conference} on},
	publisher = {IEEE},
	author = {Helgadóttir, Lovísa Irpa and Haenicke, Joachim and Landgraf, Tim and Rojas, Raul and Nawrot, Martin P.},
	year = {2013},
	keywords = {peer-reviewed},
	pages = {891--894}
}

@article{landgraf_design_2008,
	title = {Design and development of a robotic bee for the analysis of honeybee dance communication},
	volume = {5},
	url = {http://www.tandfonline.com/doi/abs/10.1080/11762320802617552},
	number = {3},
	urldate = {2016-11-29},
	journal = {Applied Bionics and Biomechanics},
	author = {Landgraf, T. and Moballegh, H. and Rojas, R.},
	year = {2008},
	keywords = {peer-reviewed},
	pages = {157--164}
}

@article{hussaini_sleep_2009,
	title = {Sleep deprivation affects extinction but not acquisition memory in honeybees},
	volume = {16},
	url = {http://learnmem.cshlp.org/content/16/11/698.short},
	number = {11},
	urldate = {2016-11-29},
	journal = {Learning \& memory},
	author = {Hussaini, Syed Abid and Bogusch, Lisa and Landgraf, Tim and Menzel, Randolf},
	year = {2009},
	keywords = {peer-reviewed},
	pages = {698--705}
}

@inproceedings{landgraf_neurocopter:_2013,
	title = {{NeuroCopter}: neuromorphic computation of {6D} ego-motion of a quadcopter},
	shorttitle = {{NeuroCopter}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-39802-5_13},
	urldate = {2016-11-29},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Landgraf, Tim and Wild, Benjamin and Ludwig, Tobias and Nowak, Philipp and Helgadottir, Lovisa and Daumenlang, Benjamin and Breinlinger, Philipp and Nawrot, Martin and Rojas, Raúl},
	year = {2013},
	keywords = {peer-reviewed},
	pages = {143--153}
}

@inproceedings{landgraf_interactive_2013,
	title = {Interactive robotic fish for the analysis of swarm behavior},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-38703-6_1},
	urldate = {2016-11-29},
	booktitle = {International {Conference} in {Swarm} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Landgraf, Tim and Nguyen, Hai and Forgo, Stefan and Schneider, Jan and Schröer, Joseph and Krüger, Christoph and Matzke, Henrik and Clément, Romain O. and Krause, Jens and Rojas, Raúl},
	year = {2013},
	keywords = {peer-reviewed},
	pages = {1--10},
	file = {[PDF] from researchgate.net:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/M3D38ERD/Landgraf et al. - 2013 - Interactive robotic fish for the analysis of swarm.pdf:application/pdf}
}

@article{jin_walking_2014,
	title = {Walking bumblebees memorize panorama and local cues in a laboratory test of navigation},
	volume = {97},
	url = {http://www.sciencedirect.com/science/article/pii/S0003347214003273},
	urldate = {2016-11-29},
	journal = {Animal Behaviour},
	author = {Jin, Nanxiang and Landgraf, Tim and Klein, Simon and Menzel, Randolf},
	year = {2014},
	keywords = {peer-reviewed},
	pages = {13--23},
	file = {[PDF] from researchgate.net:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/JA7APBNT/Jin et al. - 2014 - Walking bumblebees memorize panorama and local cue.pdf:application/pdf}
}

@inproceedings{landgraf_imitation_2012,
	title = {Imitation of the honeybee dance communication system by means of a biomimetic robot},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-31525-1_12},
	urldate = {2016-12-20},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	publisher = {Springer},
	author = {Landgraf, Tim and Oertel, Michael and Kirbach, Andreas and Menzel, Randolf and Rojas, Raúl},
	year = {2012},
	keywords = {peer-reviewed},
	pages = {132--143},
	file = {Landgraf et al._2012_Imitation of the honeybee dance communication system by means of a biomimetic robot.pdf:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/X5N679X3/Landgraf et al._2012_Imitation of the honeybee dance communication system by means of a biomimetic robot.pdf:application/pdf}
}

@article{landgraf_analysis_2011,
	title = {Analysis of the {Waggle} {Dance} {Motion} of {Honeybees} for the {Design} of a {Biomimetic} {Honeybee} {Robot}},
	volume = {6},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0021354},
	doi = {10.1371/journal.pone.0021354},
	language = {en},
	number = {8},
	urldate = {2017-10-23},
	journal = {PLoS ONE},
	author = {Landgraf, Tim and Rojas, Raúl and Nguyen, Hai and Kriegel, Fabian and Stettin, Katja},
	editor = {Krapp, Holger G.},
	month = aug,
	year = {2011},
	keywords = {peer-reviewed},
	pages = {e21354},
	file = {journal.pone.0021354.PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/74ESK9DB/journal.pone.0021354.PDF:application/pdf}
}

@techreport{landgraf_tracking_2007,
	title = {Tracking honey bee dances from sparse optical flow fields},
	url = {http://www.diss.fu-berlin.de/docs/servlets/MCRFileNodeServlet/FUDOCS_derivate_000000000829/2007_11.pdf},
	author = {Landgraf, Tim and Rojas, Raúl},
	year = {2007},
	file = {Landgraf and Rojas - 2007 - Tracking honey bee dances from sparse optical flow.pdf:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/TP64JRC9/Landgraf and Rojas - 2007 - Tracking honey bee dances from sparse optical flow.pdf:application/pdf}
}

@article{bierbach_insights_2018,
	title = {Insights into the {Social} {Behavior} of {Surface} and {Cave}-{Dwelling} {Fish} ({Poecilia} mexicana) in {Light} and {Darkness} through the {Use} of a {Biomimetic} {Robot}},
	volume = {5},
	issn = {2296-9144},
	url = {http://journal.frontiersin.org/article/10.3389/frobt.2018.00003/full},
	doi = {10.3389/frobt.2018.00003},
	urldate = {2018-02-13},
	journal = {Frontiers in Robotics and AI},
	author = {Bierbach, David and Lukas, Juliane and Bergmann, Anja and Elsner, Kristiane and Höhne, Leander and Weber, Christiane and Weimar, Nils and Arias-Rodriguez, Lenin and Mönck, Hauke J. and Nguyen, Hai and Romanczuk, Pawel and Landgraf, Tim and Krause, Jens},
	month = feb,
	year = {2018},
	keywords = {peer-reviewed}
}

@inproceedings{landgraf_blending_2014,
	title = {Blending in with the shoal: robotic fish swarms for investigating strategies of group formation in guppies},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	publisher = {Springer, Cham},
	author = {Landgraf, Tim and Nguyen, Hai and Schröer, Joseph and Szengel, Angelika and Clément, Romain JG and Bierbach, David and Krause, Jens},
	year = {2014},
	keywords = {peer-reviewed},
	pages = {178--189}
}

@inproceedings{worm_electro-communicating_2014,
	title = {Electro-communicating dummy fish initiate group behavior in the weakly electric fish {Mormyrus} rume},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	publisher = {Springer, Cham},
	author = {Worm, Martin and Landgraf, Tim and Nguyen, Hai and von der Emde, Gerhard},
	year = {2014},
	keywords = {peer-reviewed},
	pages = {446--448}
}

@inproceedings{landgraf_biomimetic_2010,
	title = {A biomimetic honeybee robot for the analysis of the honeybee dance communication system},
	booktitle = {Intelligent {Robots} and {Systems} ({IROS}), 2010 {IEEE}/{RSJ} {International} {Conference} on},
	publisher = {IEEE},
	author = {Landgraf, Tim and Oertel, Michael and Rhiel, Daniel and Rojas, Raúl},
	year = {2010},
	keywords = {peer-reviewed},
	pages = {3097--3102}
}

@inproceedings{meyer_digital_2011,
	title = {A digital receptor neuron connecting remote sensor hardware to spiking neural networks},
	booktitle = {{BC11} : {Computational} {Neuroscience} \& {Neurotechnology} {Bernstein} {Conference} \& {Neurex} {Annual} {Meeting} 2011, {Freiburg}, {Germany}, 4 {Oct} - 6 {Oct}, 2011.},
	publisher = {Frontiers Neuroscience},
	author = {Meyer, Jan and Haenicke, Joachim and Landgraf, Tim and Schmuker, Michael and Rojas, Raúl and Nawrot, Martin},
	year = {2011}
}

@inproceedings{landgraf_blending_2011,
	title = {Blending into the {Hive}: {A} {Novel} {Biomimetic} {Honeybee} {Robot} for the {Analysis} of the {Dance} {Communication} {System}.},
	booktitle = {International {Workshop} on {Bio}-{Inspired} {Robots}, {Nantes} {April} 6-8},
	publisher = {Ecole des Mines, IRCCYN LAB},
	author = {Landgraf, Tim},
	year = {2011},
	keywords = {peer-reviewed}
}

@inproceedings{helgadottir_robotic_2012,
	title = {A {Robotic} {Platform} for {Spiking} {Neural} {Control} {Architectures}},
	booktitle = {Bernstein {Conference} 2012, {Munich}, {Germany}, 12 {Sep} - 14 {Sep}, 2012.},
	publisher = {Frontiers in Computational Neuroscience},
	author = {Helgadottir, Lovisa Irpa and Haenicke, Joachim and Landgraf, Tim and Nawrot, Martin Paul},
	year = {2012},
	pages = {154}
}

@techreport{landgraf_dancing_2018,
	title = {Dancing {Honey} bee {Robot} {Elicits} {Dance}-{Following} and {Recruits} {Foragers}},
	author = {Landgraf, Tim and Bierbach, David and Kirbach, Andreas and Cusing, Rachel and Oertel, Michael and Lehmann, Konstantin and Greggers, Uwe and Menzel, Randolf and Rojas, Raúl},
	year = {2018}
}

@techreport{monck_biotracker:_2018,
	title = {{BioTracker}: {An} {Open}-{Source} {Computer} {Vision} {Framework} for {Visual} {Animal} {Tracking}},
	author = {Mönck, Hauke Jürgen and Jörg, Andreas and von Falkenhausen, Tobias and Tanke, Julian and Wild, Benjamin and Dormagen, David and Piotrowski, Jonas and Winklmayr, Claudia and Bierbach, David and Landgraf, Tim},
	year = {2018}
}

@article{lam_dancing_2017,
	title = {Dancing attraction: followers of honey bee tremble and waggle dances exhibit similar behaviors},
	journal = {Biology open},
	author = {Lam, Calvin and Li, Yanlei and Landgraf, Tim and Nieh, James},
	year = {2017},
	keywords = {peer-reviewed},
	pages = {bio--025445}
}

@incollection{landgraf_kunstliche_2017,
	title = {Künstliche {Mini}-{Gehirne} für {Roboter}},
	booktitle = {Planen und {Handeln}},
	publisher = {Springer Spektrum, Wiesbaden},
	author = {Landgraf, Tim and Nawrot, Martin},
	year = {2017},
	pages = {135--150}
}

@article{landgraf_robofish:_2016,
	title = {{RoboFish}: increased acceptance of interactive robotic fish with realistic eyes and natural motion patterns by live {Trinidadian} guppies},
	volume = {11},
	number = {1},
	journal = {Bioinspiration \& Biomimetics},
	author = {Landgraf, Tim and Bierbach, David and Nguyen, Hai and Muggelberg, Nadine and Romanczuk, Pawel and Krause, Jens},
	year = {2016},
	keywords = {peer-reviewed},
	pages = {015001},
	file = {[PDF] from researchgate.net:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/7CP7I5V2/Landgraf et al. - 2016 - RoboFish increased acceptance of interactive robo.pdf:application/pdf}
}

@article{menzel_honeybees_2018,
	title = {Honeybees are {Guided} by {Learned} {Elongated} {Ground} {Structures}},
	volume = {12},
	issn = {1662-5153},
	url = {https://www.frontiersin.org/articles/10.3389/fnbeh.2018.00322/full},
	doi = {10.3389/fnbeh.2018.00322},
	abstract = {Elongated landscape features like forest edges, rivers, roads or boundaries of fields are particularly salient landmarks for navigating animals. Here we ask how honeybees learn such structures and how they are used during their homing flights after being released at an unexpected location (catch-and-release paradigm). The experiments were performed in two landscapes that differed with respect to their overall structure: a rather feature-less landscape, and one rich in close and far distant landmarks. We tested three different forms of learning: learning during orientation flights, learning during training to a feeding site, and learning during homing flights after release at an unexpected site within the explored area. We found that bees use elongated ground structures, e.g. a field boundary separating two pastures close to the hive (experiment 1), an irrigation channel (experiment 2), a hedgerow along which the bees were trained (experiment 3), a gravel road close to the hive and the feeder (experiment 4), a path along an irrigation channel with its vegetation close to the feeder (experiment 5) and a gravel road along which bees performed their homing flights (experiment 6). Discrimination and generalization between the learned linear landmarks and similar ones in the test area depend on their object properties (irrigation channel, gravel road, hedgerow) and their compass orientation. We conclude that elongated ground structures are embedded into multiple landscape features indicating that memory of these linear structures is one component of bee navigation. Elongated structures interact and compete with other references. Object identification is an important part of this process. The objects are characterized not only by their appearance but also by their alignment in the compass. Their salience is highest if both components are close to what had been learned. High similarity in appearance can compensate for (partial) compass misalignment, and vice versa.},
	language = {English},
	urldate = {2019-01-14},
	journal = {Frontiers in Behavioral Neuroscience},
	author = {Menzel, Randolf and Tison, Lea and Fischer-Nakai, Johannes and Cheeseman, James and Lehmann, Konstantin and Balbuena, Maria Sol and Chen, Xiuxian and Landgraf, Tim and Petrasch, Julian and Greggers, Uwe},
	year = {2018},
	keywords = {object recognition, Compass alignment, ground structures, guiding landmarks, navigation, sun compass, peer-reviewed}
}

@techreport{polster_reconstructing_2018,
	title = {Reconstructing the visual perception of honey bees in complex 3-{D} worlds},
	url = {http://arxiv.org/abs/1811.07560},
	abstract = {Over the last decades, honeybees have been a fascinating model to study insect navigation. While there is some controversy about the complexity of underlying neural correlates, the research of honeybee navigation makes progress through both the analysis of flight behavior and the synthesis of agent models. Since visual cues are believed to play a crucial role for the behavioral output of a navigating bee we have developed a realistic 3-dimensional virtual world, in which simulated agents can be tested, or in which the visual input of experimentally traced animals can be reconstructed. In this paper we present implementation details on how we reconstructed a large 3-dimensional world from aerial imagery of one of our field sites, how the distribution of ommatidia and their view geometry was modeled, and how the system samples from the scene to obtain realistic bee views. This system is made available as an open-source project to the community on {\textbackslash}url\{http://github.com/bioroboticslab/bee\_view\}.},
	urldate = {2019-01-14},
	author = {Polster, Johannes and Petrasch, Julian and Menzel, Randolf and Landgraf, Tim},
	month = nov,
	year = {2018},
	keywords = {Quantitative Biology - Quantitative Methods},
	file = {arXiv\:1811.07560 PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/63TSDI5W/Polster et al. - 2018 - Reconstructing the visual perception of honey bees.pdf:application/pdf;arXiv.org Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/GX2VQGVL/1811.html:text/html}
}

@article{muller_neural_2018,
	title = {A neural network model for familiarity and context learning during honeybee foraging flights},
	volume = {112},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/s00422-017-0732-z},
	doi = {10.1007/s00422-017-0732-z},
	language = {en},
	number = {1-2},
	urldate = {2018-12-13},
	journal = {Biological Cybernetics},
	author = {Müller, Jurek and Nawrot, Martin and Menzel, Randolf and Landgraf, Tim},
	month = apr,
	year = {2018},
	keywords = {peer-reviewed},
	pages = {113--126},
	file = {Müller et al. - 2018 - A neural network model for familiarity and context.pdf:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/8C4NMSVE/Müller et al. - 2018 - A neural network model for familiarity and context.pdf:application/pdf}
}

@techreport{wild_automatic_2018,
	title = {Automatic localization and decoding of honeybee markers using deep convolutional neural networks},
	url = {http://arxiv.org/abs/1802.04557},
	abstract = {The honeybee is a fascinating model animal to investigate how collective behavior emerges from (inter-)actions of thousands of individuals. Bees may acquire unique memories throughout their lives. These experiences affect social interactions even over large time frames. Tracking and identifying all bees in the colony over their lifetimes therefore may likely shed light on the interplay of individual differences and colony behavior. This paper proposes a software pipeline based on two deep convolutional neural networks for the localization and decoding of custom binary markers that honeybees carry from their first to the last day in their life. We show that this approach outperforms similar systems proposed in recent literature. By opening this software for the public, we hope that the resulting datasets will help advancing the understanding of honeybee collective intelligence.},
	urldate = {2019-05-27},
	author = {Wild, Benjamin and Sixt, Leon and Landgraf, Tim},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1802.04557 PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/V8F9R6NX/Wild et al. - 2018 - Automatic localization and decoding of honeybee ma.pdf:application/pdf;arXiv\:1802.04557 PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/ULNMJ4B3/Wild et al. - 2018 - Automatic localization and decoding of honeybee ma.pdf:application/pdf;arXiv.org Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/N2HZQ67T/1802.html:text/html;arXiv.org Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/7BCPY4V2/1802.html:text/html;Wild et al. - 2018 - Automatic localization and decoding of honeybee ma.pdf:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/L76SKTYV/Wild et al. - 2018 - Automatic localization and decoding of honeybee ma.pdf:application/pdf}
}

@article{wario_automatic_2015,
	title = {Automatic methods for long-term tracking and the detection and decoding of communication dances in honeybees},
	volume = {3},
	issn = {2296-701X},
	doi = {10.3389/fevo.2015.00103},
	abstract = {The honeybee waggle dance communication system is an intriguing example of abstract animal communication and has been investigated thoroughly throughout the last seven decades. Typically, observables such as waggle durations or body angles are extracted manually either directly from the observation hive or from video recordings to quantify properties of the dance and related behaviors. In recent years, biology has proﬁted from automation, improving measurement precision, removing human bias, and accelerating data collection. We have developed technologies to track all individuals of a honeybee colony and to detect and decode communication dances automatically. In strong contrast to conventional approaches that focus on a small subset of the hive life, whether this regards time, space, or animal identity, our more inclusive system will help the understanding of the dance comprehensively in its spatial, temporal, and social context. In this contribution, we present full speciﬁcations of the recording setup and the software for automatic recognition of individually tagged bees and the decoding of dances. We discuss potential research directions that may beneﬁt from the proposed automation. Lastly, to exemplify the power of the methodology, we show experimental data and respective analyses from a continuous, experimental recording of 9 weeks duration.},
	language = {en},
	urldate = {2019-05-27},
	journal = {Frontiers in Ecology and Evolution},
	author = {Wario, Fernando and Wild, Benjamin and Couvillon, Margaret J. and Rojas, Raúl and Landgraf, Tim},
	month = sep,
	year = {2015},
	keywords = {animal behavior, animal tracking, honeybee, peer-reviewed, waggle dance},
	file = {Full Text PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/84S33XGB/Wario et al. - 2015 - Automatic methods for long-term tracking and the d.pdf:application/pdf;Wario et al. - 2015 - Automatic methods for long-term tracking and the d.pdf:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/5LIJ56TL/Wario et al. - 2015 - Automatic methods for long-term tracking and the d.pdf:application/pdf}
}

@article{boenisch_tracking_2018,
	title = {Tracking {All} {Members} of a {Honey} {Bee} {Colony} {Over} {Their} {Lifetime} {Using} {Learned} {Models} of {Correspondence}},
	volume = {5},
	issn = {2296-9144},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2018.00035/full},
	doi = {10.3389/frobt.2018.00035},
	abstract = {Computational approaches to the analysis of collective behavior in social insects increasingly rely on motion paths as an intermediate data layer from which one can infer individual behaviors or social interactions. Honey bees are a popular model for learning and memory. Previous experience has been shown to affect and modulate future social interactions. So far, no lifetime history observations have been reported for all bees of a colony. In a previous work we introduced a recording setup customized to track up to 4000 marked bees over several weeks. Due to detection and decoding errors of the bee markers, linking the correct correspondences through time is non-trivial. In this contribution we present an in-depth description of the underlying multi-step algorithm which produces motion paths, and also improves the marker decoding accuracy significantly. The proposed solution employs two classifiers to predict the correspondence of two consecutive detections in the first step, and two tracklets in the second. We automatically tracked {\textasciitilde}2000 marked honey bees over 10 weeks with inexpensive recording hardware using markers without any error correction bits. We found that the proposed two-step tracking reduced incorrect ID decodings from initially {\textasciitilde}13 \% to around 2 \% post-tracking. Alongside this paper, we publish the first trajectory dataset for all bees in a colony, extracted from {\textasciitilde}3 million images covering three days. We invite researchers to join the collective scientific effort to investigate this intriguing animal system. All components of our system are open-source.},
	language = {English},
	urldate = {2019-05-27},
	journal = {Frontiers in Robotics and AI},
	author = {Boenisch, Franziska and Rosemann, Benjamin and Wild, Benjamin and Dormagen, David and Wario, Fernando and Landgraf, Tim},
	year = {2018},
	note = {tex.ids= boenisch\_tracking\_2018},
	keywords = {Apis mellifera, honey bees, Lifetime History, peer-reviewed, social insects, tracking, trajectory},
	file = {Full Text PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/NTWTRNTZ/Boenisch et al. - 2018 - Tracking All Members of a Honey Bee Colony Over Th.pdf:application/pdf;Full Text PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/YSN3WRT6/Boenisch et al. - 2018 - Tracking All Members of a Honey Bee Colony Over Th.pdf:application/pdf}
}

@article{bierbach_using_2018,
	title = {Using a robotic fish to investigate individual differences in social responsiveness in the guppy},
	volume = {5},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.181026},
	doi = {10.1098/rsos.181026},
	abstract = {Responding towards the actions of others is one of the most important behavioural traits whenever animals of the same species interact. Mutual influences among interacting individuals may modulate the social responsiveness seen and thus make it often difficult to study the level and individual variation in responsiveness. Here, open-loop biomimetic robots that provide standardized, non-interactive social cues can be a useful tool. These robots are not affected by the live animal's actions but are assumed to still represent valuable and biologically relevant social cues. As this assumption is crucial for the use of biomimetic robots in behavioural studies, we hypothesized (i) that meaningful social interactions can be assumed if live animals maintain individual differences in responsiveness when interacting with both a biomimetic robot and a live partner. Furthermore, to study the level of individual variation in social responsiveness, we hypothesized (ii) that individual differences should be maintained over the course of multiple tests with the robot. We investigated the response of live guppies (Poecilia reticulata) when allowed to interact either with a biomimetic open-loop-controlled fish robot—‘Robofish’—or with a live companion. Furthermore, we investigated the responses of live guppies when tested three times with Robofish. We found that responses of live guppies towards Robofish were weaker compared with those of a live companion, most likely as a result of the non-interactive open-loop behaviour of Robofish. Guppies, however, were consistent in their individual responses between a live companion and Robofish, and similar individual differences in response towards Robofish were maintained over repeated testing even though habituation to the test environment was detectable. Biomimetic robots like Robofish are therefore a useful tool for the study of social responsiveness in guppies and possibly other small fish species.},
	number = {8},
	urldate = {2019-02-11},
	journal = {Royal Society Open Science},
	author = {Bierbach, David and Landgraf, Tim and Romanczuk, Pawel and Lukas, Juliane and Nguyen, Hai and Wolf, Max and Krause, Jens},
	year = {2018},
	keywords = {peer-reviewed},
	pages = {181026},
	file = {Full Text PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/7HVL42GR/Bierbach David et al. - Using a robotic fish to investigate individual dif.pdf:application/pdf;Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/RUVXW6RG/rsos.html:text/html}
}

@article{worm_evidence_2018,
	title = {Evidence for mutual allocation of social attention through interactive signaling in a mormyrid weakly electric fish},
	volume = {115},
	copyright = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/26/6852},
	doi = {10.1073/pnas.1801283115},
	abstract = {Mormyrid weakly electric fish produce electric organ discharges (EODs) for active electrolocation and electrocommunication. These pulses are emitted with variable interdischarge intervals (IDIs) resulting in temporal discharge patterns and interactive signaling episodes with nearby conspecifics. However, unequivocal assignment of interactive signaling to a specific behavioral context has proven to be challenging. Using an ethorobotical approach, we confronted single individuals of weakly electric Mormyrus rume proboscirostris with a mobile fish robot capable of interacting both physically, on arbitrary trajectories, as well as electrically, by generating echo responses through playback of species-specific EODs, thus synchronizing signals with the fish. Interactive signaling by the fish was more pronounced in response to a dynamic echo playback generated by the robot than in response to playback of static random IDI sequences. Such synchronizations were particularly strong at a distance corresponding to the outer limit of active electrolocation, and when fish oriented toward the fish replica. We therefore argue that interactive signaling through echoing of a conspecific’s EODs provides a simple mechanism by which weakly electric fish can specifically address nearby individuals during electrocommunication. Echoing may thus enable mormyrids to mutually allocate social attention and constitute a foundation for complex social behavior and relatively advanced cognitive abilities in a basal vertebrate lineage.},
	language = {en},
	number = {26},
	urldate = {2019-10-01},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Worm, Martin and Landgraf, Tim and Prume, Julia and Nguyen, Hai and Kirschbaum, Frank and Emde, Gerhard von der},
	month = jun,
	year = {2018},
	keywords = {electrocommunication, ethorobotics, interactive signaling, social attention, peer-reviewed},
	pages = {6852--6857},
	file = {Full Text PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/TIHK2EP2/Worm et al. - 2018 - Evidence for mutual allocation of social attention.pdf:application/pdf}
}

@misc{paffhausen_neural_2019,
	address = {Göttingen},
	title = {Neural correlates of mushroom body output neurons measured during flight of a harnessed honey bee on a quad copter},
	abstract = {Honey bee navigation is an actively investigated field but the knowledge about the neural correlates of goal directed long distance navigation remains mostly unknown. We recorded single neuron activity during flight in a natural environment. Bees were trained to a feeder 400 m from the hive. Spiking activity of high order interneurons (mushroom body extrinsic neurons of the A3 cluster) were recorded with extracellular electrodes at the alpha exit. The bees were attached to a quad copter together with the necessary amplifiers and data storing devices. The copter flew along the path the bees had taken during training to the feeder. Additional flight paths were flown at natural speed and height. The spike rates of the recorded neurons were analyzed with respect to the corresponding flight tracks. Preliminary analyses of the data showed a strong and repeated spike rate change whenever the copter turned in tight bends. Straight flights resulted in partially repeated spike rate changes along similar stretches of the flight path but with much higher variance. Further analyses are on the way. The next experiments will include other flight paths focusing on the question whether spiking activity can be related to object identification and localization, properties that are considered to be involved in mushroom body function.},
	author = {Paffhausen, Benjamin and Petrasch, Julian and Wild, Benjamin and Fuchs, Inga and Drexler, Helmut and Kuriatnyk, Oleksandra and Meurers, Thierry and Landgraf, Tim and Menzel, Randolf},
	month = jun,
	year = {2019},
	file = {Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/VXQP5I7L/331981513_Neural_correlates_of_mushroom_body_output_neurons_measured_during_flight_of_a_harness.html:text/html}
}

@inproceedings{schulz_restricting_2019,
	title = {Restricting the {Flow}: {Information} {Bottlenecks} for {Attribution}},
	shorttitle = {Restricting the {Flow}},
	url = {https://openreview.net/forum?id=S1xWh1rYwB},
	abstract = {Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual...},
	urldate = {2020-03-16},
	author = {Schulz, Karl and Sixt, Leon and Tombari, Federico and Landgraf, Tim},
	month = sep,
	year = {2019},
	keywords = {peer-reviewed},
	file = {Full Text PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/DF4CUCA4/Schulz et al. - 2019 - Restricting the Flow Information Bottlenecks for .pdf:application/pdf;Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/LR87CA6H/forum.html:text/html}
}

@misc{noauthor_frontiers_nodate,
	title = {Frontiers {\textbar} {Unsupervised} {Neural} {Coding} of {Nightingale} {Songs} {Using} {Deep} {Autoencoders}},
	url = {https://www.frontiersin.org/10.3389/conf.fncom.2011.53.00078/event_abstract},
	abstract = {In this work we tested different deep autoencoder networks for the unsupervised feature extraction of nightingale song spectra. Nightingales are versatile singers. Some individuals can sing up to 600 different songs. Biologists analyzing nightingale communication have to classify audio recordings by comparing hundreds of unknown songs to a database. This work is done mostly manually by matching spectrograms visually. An automatic or semi-automatic method for song classification would speed up this tedious process. We have utilized a recently published learning method to train multi-layered (‘deep’) artificial neural networks to reduce the dimensionality of – and find correlations within the spectrogram data in an unsupervised manner. We propose several preprocessing steps and network topologies to find low dimensional representations of nightingale songs. First, the audio data is filtered with a band pass to reduce low-frequency noise, e.g. of nearby cars. Then, we normalize the volume and down-scale the spectrograms to 256 x 400 points. This matrix is used as the input layer to the network. The next layer extracts visual features like edges and corner points. Each neuron in that layer serves as a feature detector and shares its incoming weights with different ‘receptive fields’ in the input layer and thus establishes repetition- and shift invariance. The output of this layer will be fed to the next three layers that serve for the dimensionality reduction and are trained as proposed by Hinton (2006). The weights of the network are tuned by comparing the input of the network to its reconstruction: By feeding an input song to the network, a specific code can be read from the last, 16-dimensional, code layer. By projecting back the activity of this layer to the receptive field, using the same weights, it is possible to reconstruct its original excitation; a procedure we use also to measure the quality of the code. Once the training is complete it is possible to classify unknown songs using the low dimensional code with an additional classification layer or other standard classification methods.},
	urldate = {2020-04-13}
}

@techreport{landgraf_socially_2020,
	title = {Socially competent robots: adaptation improves leadership performance in groups of live fish},
	shorttitle = {Socially competent robots},
	url = {http://arxiv.org/abs/2009.06633},
	abstract = {Collective motion is commonly modeled with simple interaction rules between agents. Yet in nature, numerous observables vary within and between individuals and it remains largely unknown how animals respond to this variability, and how much of it may be the result of social responses. Here, we hypothesize that Guppies ({\textbackslash}textit\{Poecilia reticulata\}) respond to avoidance behaviors of their shoal mates and that "socially competent" responses allow them to be more effective leaders. We test this hypothesis in an experimental setting in which a robotic Guppy, called RoboFish, is programmed to adapt to avoidance reactions of its live interaction partner. We compare the leadership performance between socially competent robots and two non-competent control behaviors and find that 1) behavioral variability itself appears attractive and that socially competent robots are better leaders that 2) require fewer approach attempts to 3) elicit longer average following behavior than non-competent agents. This work provides evidence that social responsiveness to avoidance reactions plays a role in the social dynamics of guppies. We showcase how social responsiveness can be modeled and tested directly embedded in a living animal model using adaptive, interactive robots.},
	urldate = {2020-09-17},
	author = {Landgraf, Tim and Moenck, Hauke J. and Gebhardt, Gregor H. W. and Weimar, Nils and Hocke, Mathis and Maxeiner, Moritz and Musiolek, Lea and Krause, Jens and Bierbach, David},
	month = sep,
	year = {2020},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/67WF3SFG/Landgraf et al. - 2020 - Socially competent robots adaptation improves lea.pdf:application/pdf;arXiv.org Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/F53AW34L/2009.html:text/html}
}

@inproceedings{sixt_when_2020,
	title = {When {Explanations} {Lie}: {Why} {Many} {Modified} {BP} {Attributions} {Fail}},
	volume = {1},
	shorttitle = {When {Explanations} {Lie}},
	url = {https://proceedings.icml.cc/paper/2020/hash/af21d0c97db2e27e13572cbf59eb343d},
	language = {en},
	urldate = {2020-09-17},
	booktitle = {Proceedings of the {International} {Conference} on {Machine} {Learning}},
	author = {Sixt, Leon and Granz, Maximilian and Landgraf, Tim},
	year = {2020},
	keywords = {peer-reviewed},
	file = {Full Text PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/YIT8BYJJ/Sixt et al. - 2020 - When Explanations Lie Why Many Modified BP Attrib.pdf:application/pdf;Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/D43FPAPU/af21d0c97db2e27e13572cbf59eb343d.html:text/html}
}

@article{bierbach_guppies_2020,
	title = {Guppies {Prefer} to {Follow} {Large} ({Robot}) {Leaders} {Irrespective} of {Own} {Size}},
	volume = {8},
	issn = {2296-4185},
	url = {https://www.frontiersin.org/articles/10.3389/fbioe.2020.00441/full},
	doi = {10.3389/fbioe.2020.00441},
	abstract = {Body size is often assumed to determine how successful an individual can lead others with larger individuals being better leaders than smaller ones. But even if larger individuals are more readily followed, body size often correlates with specific behavioral patterns and it is thus unclear whether larger individuals are more often followed than smaller ones because of their size or because they behave in a certain way. To control for behavioral differences among differentially-sized leaders, we used biomimetic robotic fish (Robofish) of different sizes. Live guppies (Poecilia reticulata) are known to interact with Robofish in a similar way as with live conspecifics. Consequently, Robofish may serve as a conspecific-like leader that provides standardized behaviors irrespective of its size. We asked whether larger Robofish leaders are preferentially followed and whether the preferences of followers depend on own body size or risk-taking behavior (‘boldness’). We found that live guppies followed larger Robofish leaders in closer proximity than smaller ones and this pattern was independent of the followers’ own body size as well as risk-taking behavior. Our study shows a ‘bigger is better’ pattern in leadership that is fully independent of behavioral differences among differentially-sized leaders, followers’ own size and risk-taking behavior.},
	language = {English},
	urldate = {2020-09-17},
	journal = {Frontiers in Bioengineering and Biotechnology},
	author = {Bierbach, David and Mönck, Hauke J. and Lukas, Juliane and Habedank, Marie and Romanczuk, Pawel and Landgraf, Tim and Krause, Jens},
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {biomimetic robots, peer-reviewed, Body Size, Leadership, Poecilia reticulata, Robotic fish},
	file = {Full Text PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/I7PZ2KDL/Bierbach et al. - 2020 - Guppies Prefer to Follow Large (Robot) Leaders Irr.pdf:application/pdf}
}

@techreport{sixt_rendergan:_2017,
	title = {{RenderGAN}: {Generating} {Realistic} {Labeled} {Data}},
	shorttitle = {{RenderGAN}},
	url = {http://arxiv.org/abs/1611.01331},
	abstract = {Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines.},
	urldate = {2020-09-19},
	author = {Sixt, Leon and Wild, Benjamin and Landgraf, Tim},
	month = jan,
	year = {2017},
	note = {tex.ids= sixt\_rendergan:\_2016
arXiv: 1611.01331},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, GAN},
	file = {[PDF] from arxiv.org:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/U9IK2JRA/Sixt et al. - 2016 - RenderGAN Generating Realistic Labeled Data.pdf:application/pdf;arXiv Fulltext PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/TG5Y5XMR/Sixt et al. - 2017 - RenderGAN Generating Realistic Labeled Data.pdf:application/pdf;arXiv\:1611.01331 PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/6MNB3QP2/Sixt et al. - 2016 - RenderGAN Generating Realistic Labeled Data.pdf:application/pdf;arXiv\:1611.01331 PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/W3IPQKCY/Sixt et al. - 2016 - RenderGAN Generating Realistic Labeled Data.pdf:application/pdf;arXiv.org Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/LTB749VB/1611.html:text/html;arXiv.org Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/QVVAUA6F/1611.html:text/html;arXiv.org Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/8NGCRGYK/1611.html:text/html;Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/62H28EUD/Sixt et al. - 2016 - RenderGAN Generating Realistic Labeled Data.html:text/html}
}

@article{wario_automatic_2017,
	title = {Automatic detection and decoding of honey bee waggle dances},
	volume = {12},
	issn = {1932-6203},
	url = {http://arxiv.org/abs/1708.06590},
	doi = {10.1371/journal.pone.0188626},
	abstract = {The waggle dance is one of the most popular examples of animal communication. Forager bees direct their nestmates to profitable resources via a complex motor display. Essentially, the dance encodes the polar coordinates to the resource in the field. Unemployed foragers follow the dancer's movements and then search for the advertised spots in the field. Throughout the last decades, biologists have employed different techniques to measure key characteristics of the waggle dance and decode the information it conveys. Early techniques involved the use of protractors and stopwatches to measure the dance orientation and duration directly from the observation hive. Recent approaches employ digital video recordings and manual measurements on screen. However, manual approaches are very time-consuming. Most studies, therefore, regard only small numbers of animals in short periods of time. We have developed a system capable of automatically detecting, decoding and mapping communication dances in real-time. In this paper, we describe our recording setup, the image processing steps performed for dance detection and decoding and an algorithm to map dances to the field. The proposed system performs with a detection accuracy of 90.07{\textbackslash}\%. The decoded waggle orientation has an average error of -2.92\{{\textbackslash}deg\} (\${\textbackslash}pm\$ 7.37\{{\textbackslash}deg\} ), well within the range of human error. To evaluate and exemplify the system's performance, a group of bees was trained to an artificial feeder, and all dances in the colony were automatically detected, decoded and mapped. The system presented here is the first of this kind made publicly available, including source code and hardware specifications. We hope this will foster quantitative analyses of the honey bee waggle dance.},
	number = {12},
	urldate = {2020-09-19},
	journal = {PLOS ONE},
	author = {Wario, Fernando and Wild, Benjamin and Rojas, Raúl and Landgraf, Tim},
	month = dec,
	year = {2017},
	note = {tex.ids: wario\_automatic\_2017
arXiv: 1708.06590},
	keywords = {Quantitative Biology - Quantitative Methods, Computer Science - Computer Vision and Pattern Recognition, peer-reviewed},
	pages = {e0188626},
	annote = {Comment: 14 pages, LaTeX; a new value for the ratio distance-waggle run duration was computed. Figure 6 has been updated using the newly calculated value},
	annote = {Comment: 16 pages, LaTeX; a new value for the ratio distance-waggle run duration was computed. Figure 2 has been updated and discussion section was improved},
	file = {arXiv Fulltext PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/DQ8SHPPE/Wario et al. - 2017 - Automatic detection and decoding of honey bee wagg.pdf:application/pdf;arXiv\:1708.06590 PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/ZAXSM337/Wario et al. - 2017 - Automatic detection and decoding of honey bee wagg.pdf:application/pdf;arXiv.org Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/2C2K56IR/1708.html:text/html;arXiv.org Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/LEA5ML6P/1708.html:text/html}
}

@techreport{boenisch_tracking_2018-1,
	title = {Tracking all members of a honey bee colony over their lifetime},
	url = {http://arxiv.org/abs/1802.03192},
	abstract = {Computational approaches to the analysis of collective behavior in social insects increasingly rely on motion paths as an intermediate data layer from which one can infer individual behaviors or social interactions. Honey bees are a popular model for learning and memory. Previous experience has been shown to affect and modulate future social interactions. So far, no lifetime history observations have been reported for all bees of a colony. In a previous work we introduced a tracking system customized to track up to \$4000\$ bees over several weeks. In this contribution we present an in-depth description of the underlying multi-step algorithm which both produces the motion paths, and also improves the marker decoding accuracy significantly. We automatically tracked \$\{{\textbackslash}sim\}2000\$ marked honey bees over 10 weeks with inexpensive recording hardware using markers without any error correction bits. We found that the proposed two-step tracking reduced incorrect ID decodings from initially \$\{{\textbackslash}sim\}13{\textbackslash}\%\$ to around \$2{\textbackslash}\%\$ post-tracking. Alongside this paper, we publish the first trajectory dataset for all bees in a colony, extracted from \$\{{\textbackslash}sim\} 4\$ million images. We invite researchers to join the collective scientific effort to investigate this intriguing animal system. All components of our system are open-source.},
	urldate = {2020-09-19},
	author = {Boenisch, Franziska and Rosemann, Benjamin and Wild, Benjamin and Wario, Fernando and Dormagen, David and Landgraf, Tim},
	month = mar,
	year = {2018},
	note = {arXiv: 1802.03192},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/JE5ESX8Q/Boenisch et al. - 2018 - Tracking all members of a honey bee colony over th.pdf:application/pdf;arXiv.org Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/7B9IXFX9/1802.html:text/html}
}

@techreport{sixt_when_2020-1,
	title = {When {Explanations} {Lie}: {Why} {Many} {Modified} {BP} {Attributions} {Fail}},
	shorttitle = {When {Explanations} {Lie}},
	url = {http://arxiv.org/abs/1912.09818},
	abstract = {Attribution methods aim to explain a neural network's prediction by highlighting the most relevant image areas. A popular approach is to backpropagate (BP) a custom relevance score using modified rules, rather than the gradient. We analyze an extensive set of modified BP methods: Deep Taylor Decomposition, Layer-wise Relevance Propagation (LRP), Excitation BP, PatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. We find empirically that the explanations of all mentioned methods, except for DeepLIFT, are independent of the parameters of later layers. We provide theoretical insights for this surprising behavior and also analyze why DeepLIFT does not suffer from this limitation. Empirically, we measure how information of later layers is ignored by using our new metric, cosine similarity convergence (CSC). The paper provides a framework to assess the faithfulness of new and existing modified BP methods theoretically and empirically. For code see: https://github.com/berleon/when-explanations-lie},
	urldate = {2020-09-19},
	author = {Sixt, Leon and Granz, Maximilian and Landgraf, Tim},
	month = aug,
	year = {2020},
	note = {arXiv: 1912.09818},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: Published in ICML 2020, Camera Ready Version},
	file = {arXiv Fulltext PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/E6EGXH2H/Sixt et al. - 2020 - When Explanations Lie Why Many Modified BP Attrib.pdf:application/pdf;arXiv.org Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/ZQVC88VR/1912.html:text/html}
}

@techreport{schulz_restricting_2020,
	title = {Restricting the {Flow}: {Information} {Bottlenecks} for {Attribution}},
	shorttitle = {Restricting the {Flow}},
	url = {http://arxiv.org/abs/2001.00396},
	abstract = {Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. In this work we adapt the information bottleneck concept for attribution. By adding noise to intermediate feature maps we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods outperform all baselines in five out of six settings. The method's information-theoretic foundation provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored close to zero are not necessary for the network's decision. For reviews: https://openreview.net/forum?id=S1xWh1rYwB For code: https://github.com/BioroboticsLab/IBA},
	urldate = {2020-09-19},
	author = {Schulz, Karl and Sixt, Leon and Tombari, Federico and Landgraf, Tim},
	month = may,
	year = {2020},
	note = {arXiv: 2001.00396},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 18 pages, 12 figures, accepted at ICLR 2020 (Oral)},
	file = {arXiv Fulltext PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/EC7XZ8P2/Schulz et al. - 2020 - Restricting the Flow Information Bottlenecks for .pdf:application/pdf;arXiv.org Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/28IXW5D3/2001.html:text/html}
}

@techreport{wild_social_2020,
	title = {Social networks predict the life and death of honey bees},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.05.06.076943v1},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}In many social systems, an individual’s role is reflected by its interactions with other members of the group (Gordon 2010, Pinter-Wollmann et al. 2014, Krause 2015, Farine \&amp; Whitehead 2015). In honey bee colonies (\textit{Apis mellifera}), workers generally perform different tasks as they age, yet there is high behavioral variation in same-aged bees (Seeley 1982, Robinson 1992, Huang and Robinson 1996, Johnson 2010). It is unknown how social interactions within the colony relate to an individual’s tasks throughout her life. We propose a new method to extract a single number from each individual’s interaction patterns in multimodal social networks that captures her current role in the colony. This “network age” is better than biological age at predicting task allocation (+99\%), survival (+157\%), and activity patterns (+44-108\%) and even predicts task allocation up to one week (around a sixth of her typical lifespan) into the future. Network age identifies distinct developmental paths and task changes throughout a bee’s life: We show that individuals change tasks gradually and exhibit high task repeatability, and that same aged bees form stable behavioral subgroups in which they predominantly interact with one another. While we derived interaction networks by automatically tracking a fully tagged colony, we show that tracking only 5\% of the bees is sufficient to extract a meaningful representation of the individuals’ interaction patterns, demonstrating the feasibility of our method for detecting complex social structures with reduced experimental effort. Since network age more accurately predicts task allocation than biological age, it could be used in experimental manipulations to quantify shifts in the timing of task transitions as a response. We extend our method to extract interaction patterns relevant to other attributes of the individuals, such as their mortality, opening up a broad range of possible applications. Our approach is a scalable instrument to study individual behavior through the lens of social interactions over time in honey bees and other complex social systems.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-09-19},
	author = {Wild, Benjamin and Dormagen, David M. and Zachariae, Adrian and Smith, Michael L. and Traynor, Kirsten S. and Brockmann, Dirk and Couzin, Iain D. and Landgraf, Tim},
	month = may,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.05.06.076943},
	file = {Full Text PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/3W7583ZX/Wild et al. - 2020 - Social networks predict the life and death of hone.pdf:application/pdf;Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/XHQQ2922/2020.05.06.html:text/html}
}

@article{sixt_rendergan:_2018,
	title = {{RenderGAN}: {Generating} {Realistic} {Labeled} {Data}},
	volume = {5},
	issn = {2296-9144},
	shorttitle = {{RenderGAN}},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2018.00066/full},
	doi = {10.3389/frobt.2018.00066},
	abstract = {Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines.},
	language = {English},
	urldate = {2021-03-02},
	journal = {Frontiers in Robotics and AI},
	author = {Sixt, Leon and Wild, Benjamin and Landgraf, Tim},
	year = {2018},
	note = {tex.ids= sixt\_rendergan:\_2018
publisher: Frontiers},
	keywords = {deep learning, unsupervised learning, social insects, peer-reviewed, Generative Adversarial Networks, markers},
	file = {Full Text PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/GXKHENIN/Sixt et al. - 2018 - RenderGAN Generating Realistic Labeled Data.pdf:application/pdf}
}

@article{wild_social_2021,
	title = {Social networks predict the life and death of honey bees},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-21212-5},
	doi = {10.1038/s41467-021-21212-5},
	abstract = {In complex societies, individuals’ roles are reflected by interactions with other conspecifics. Honey bees (Apis mellifera) generally change tasks as they age, but developmental trajectories of individuals can vary drastically due to physiological and environmental factors. We introduce a succinct descriptor of an individual’s social network that can be obtained without interfering with the colony. This ‘network age’ accurately predicts task allocation, survival, activity patterns, and future behavior. We analyze developmental trajectories of multiple cohorts of individuals in a natural setting and identify distinct developmental pathways and critical life changes. Our findings suggest a high stability in task allocation on an individual level. We show that our method is versatile and can extract different properties from social networks, opening up a broad range of future studies. Our approach highlights the relationship of social interactions and individual traits, and provides a scalable technique for understanding how complex social systems function.},
	language = {en},
	number = {1},
	urldate = {2021-02-17},
	journal = {Nature Communications},
	author = {Wild, Benjamin and Dormagen, David M. and Zachariae, Adrian and Smith, Michael L. and Traynor, Kirsten S. and Brockmann, Dirk and Couzin, Iain D. and Landgraf, Tim},
	month = feb,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1110},
	file = {Full Text PDF:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/HLIDDXF4/Wild et al. - 2021 - Social networks predict the life and death of hone.pdf:application/pdf;Snapshot:/home/leon/.zotero/zotero/sv2dwpiy.default/zotero/storage/U5GHA4FN/s41467-021-21212-5.html:text/html}
}
